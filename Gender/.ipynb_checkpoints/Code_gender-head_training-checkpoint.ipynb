{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import albumentations.pytorch as Ap\n",
    "import torch.nn.functional as F\n",
    "from torch import cuda\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.datasets.folder import default_loader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "import PIL\n",
    "import pandas\n",
    "from os.path import join\n",
    "import copy\n",
    "import timm \n",
    "from collections import OrderedDict\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "data_dir = '/storage_labs/3030/BelyakovM/Face_attributes/ds/db_BuevichP/gender'\n",
    "\n",
    "from Datasets.MyDataset_gender import MyDataset_gender\n",
    "\n",
    "image_datasets = {x: MyDataset_gender(data_dir,x,list(range(2)),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=64,\n",
    "                                             shuffle=False, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: image_datasets[x].__len__() for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train loop for gender trainig \n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)[0]\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We want to gradually mount different face attributes heads on a face-recognition model and for this reason we have to be able to retain backbone recognition weights unchanged. Further we are going to initialize model with gender trained beforehand weights, freeze target weights, train it a bit and then check whether infered test images give the same prediction tensor  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mit-han-lab_ProxylessNAS_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initializing model \n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates a MTL model with the encoder from \"model_backbone\" \n",
    "    \"\"\"\n",
    "    def __init__(self, model_backbone):\n",
    "        super(MultiTaskModel,self).__init__()\n",
    "        self.encoder = model_backbone       #fastai function that creates an encoder given an architecture\n",
    "        self.fc1 = nn.Linear(in_features=1432, out_features=2, bias=True)    #fastai function that creates a head\n",
    "        self.fc2 = nn.Linear(in_features=1432, out_features=90, bias=True)\n",
    "        self.fc3 = nn.Linear(in_features=1432, out_features=7, bias=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        gender = self.fc1(x)\n",
    "        age = self.fc2(x)\n",
    "        emotions = self.fc3(x)\n",
    "\n",
    "        return [age, gender, emotions]\n",
    "class MultiTaskModel_grouped_age_head(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates a MTL model with the encoder from \"model_backbone\" \n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        super(MultiTaskModel_grouped_age_head,self).__init__()\n",
    "        self.encoder = model     \n",
    "        self.idx_tensor = torch.from_numpy(np.array([idx for idx in range(31)])).cuda()\n",
    "        self.age_group_head = nn.Linear(in_features=1400, out_features=31, bias=True)\n",
    "        self.Softmax = nn.Softmax(1)\n",
    "    def forward(self,x):\n",
    "\n",
    "        age,gender,emotions = self.encoder(x)\n",
    "\n",
    "        grouped_age = self.age_group_head(age)\n",
    "        regression_age = torch.sum(self.Softmax(grouped_age) * self.idx_tensor, axis=1)*3\n",
    "  \n",
    "\n",
    "        return [gender, (grouped_age,regression_age),  emotions]\n",
    "\n",
    "\n",
    "model_ft = torch.hub.load('mit-han-lab/ProxylessNAS', \"proxyless_cpu\" , pretrained=True)\n",
    "model_ft.classifier = nn.Sequential(*list(model_ft.classifier.children())[:-3])\n",
    "model_ft = MultiTaskModel(model_ft)\n",
    "model_ft.fc2 = nn.Linear(in_features=1432, out_features=1400, bias=True)\n",
    "model_ft = MultiTaskModel_grouped_age_head(model_ft)\n",
    "model_ft.load_state_dict(torch.load('/storage_labs/3030/BelyakovM/Face_attributes/Saved_models/proxyless-cpu_gender_age_trained.pth',map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.fc2.weight\n",
      "encoder.fc2.bias\n",
      "encoder.fc3.weight\n",
      "encoder.fc3.bias\n",
      "age_group_head.weight\n",
      "age_group_head.bias\n"
     ]
    }
   ],
   "source": [
    "#freezing all the parameters and batchnorms except parameters of age and emotion heads\n",
    "for module in model_ft.encoder.modules():\n",
    "    if isinstance(module,nn.modules.BatchNorm1d):\n",
    "        module.eval()\n",
    "    if isinstance(module,nn.modules.BatchNorm2d):\n",
    "        module.eval()\n",
    "    if isinstance(module,nn.modules.BatchNorm3d):\n",
    "        module.eval()\n",
    "for i in model_ft.parameters():\n",
    "    i.requires_grad = False\n",
    "for param in model_ft.encoder.fc2.parameters():\n",
    "    param.requires_grad = True\n",
    "for k in model_ft.encoder.fc3.parameters():\n",
    "    k.requires_grad = True\n",
    "for k in model_ft.age_group_head.parameters():\n",
    "    k.requires_grad = True\n",
    "for name,param in model_ft.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further we verify that we frozen all the backbone gender-trained and gender-head weights. It stands for the reason that if the weights are frozen after train procedure we should get the same output prediction tensor on our 2 test images(tensor 1:[[-0.1700,  0.4067]], tensor 2:[[0.1195, 0.4292]])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "----------\n",
      "train Loss: 0.0845 Acc: 0.9665\n",
      "val Loss: 0.0772 Acc: 0.9727\n",
      "\n",
      "Epoch 1/1\n",
      "----------\n",
      "train Loss: 0.0801 Acc: 0.9679\n",
      "val Loss: 0.0667 Acc: 0.9766\n",
      "\n",
      "Training complete in 43m 27s\n",
      "Best val Acc: 0.976599\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "model_ft.to(device)\n",
    "\n",
    "model_ft = train_model(model_ft, criterion,optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1700,  0.4067]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#Verifing on 1st image that all the gender-trained parameters haven't changed. \n",
    "tensor = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "pic = PIL.Image.open('/storage_labs/3030/BelyakovM/FaceMask_presence/ds/train/wo_mask/sibur/0/1_2020-07-10_19-18-18.jpg')\n",
    "#pic = data_transforms['val'](image = pic)\n",
    "pic = tensor(pic)\n",
    "pic = pic.unsqueeze(0)\n",
    "pic = pic.to(device)\n",
    "model_ft.to(device)\n",
    "outputs = model_ft(pic)\n",
    "_, preds = torch.max(outputs[0], 1)\n",
    "\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1195, 0.4292]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#Verifing on 2nd image that all the gender-trained parameters haven't changed\n",
    "tensor = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "pic = PIL.Image.open('/storage_labs/3030/BelyakovM/FaceMask_presence/ds/train/wo_mask/sibur/0/4853_2020-08-12_09-49-32.jpg')\n",
    "pic = tensor(pic)\n",
    "pic = pic.unsqueeze(0)\n",
    "pic = pic.to(device)\n",
    "model_ft.to(device)\n",
    "outputs = model_ft(pic)\n",
    "_, preds = torch.max(outputs[0], 1)\n",
    "\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the desired weights were frozen successfully "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
