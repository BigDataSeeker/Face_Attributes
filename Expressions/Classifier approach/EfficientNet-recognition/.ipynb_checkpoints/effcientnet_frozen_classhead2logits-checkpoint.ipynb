{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.datasets.folder import default_loader\n",
    "import math\n",
    "import collections\n",
    "from functools import partial\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import copy\n",
    "from torch.utils import model_zoo\n",
    "from torch.nn import Sequential, BatchNorm1d, BatchNorm2d, Dropout, Module, Linear\n",
    "import yaml\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = argparse.ArgumentParser(description='traditional_training for face recognition.')\n",
    "\n",
    "conf.add_argument(\"--backbone_type\", type = str,default = 'EfficientNet',\n",
    "                      help = \"Mobilefacenets, Resnet.\")\n",
    "conf.add_argument(\"--backbone_conf_file\", type = str ,default ='/storage_labs/3030/BelyakovM/Face_attributes/Code/EfficientNet_B0_face_recognizer/FaceX-Zoo/training_mode/backbone_conf.yaml', \n",
    "                      help = \"the path of backbone_conf.yaml.\")\n",
    "conf.add_argument(\"--head_type\", type = str ,default = 'AdaM-Softmax', \n",
    "                      help = \"mv-softmax, arcface, npc-face.\")\n",
    "conf.add_argument(\"--head_conf_file\", type = str ,default = '/storage_labs/3030/BelyakovM/Face_attributes/Code/EfficientNet_B0_face_recognizer/FaceX-Zoo/training_mode/head_conf.yaml', \n",
    "                      help = \"the path of head_conf.yaml.\")\n",
    "    \n",
    "args = conf.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_weights(model, model_name, weights_path=None, load_fc=True, advprop=False):\n",
    "    \"\"\"Loads pretrained weights from weights path or download using url.\n",
    "    Args:\n",
    "        model (Module): The whole model of efficientnet.\n",
    "        model_name (str): Model name of efficientnet.\n",
    "        weights_path (None or str):\n",
    "            str: path to pretrained weights file on the local disk.\n",
    "            None: use pretrained weights downloaded from the Internet.\n",
    "        load_fc (bool): Whether to load pretrained weights for fc layer at the end of the model.\n",
    "        advprop (bool): Whether to load pretrained weights\n",
    "                        trained with advprop (valid when weights_path is None).\n",
    "    \"\"\"\n",
    "    if isinstance(weights_path, str):\n",
    "        state_dict = torch.load(weights_path)['state_dict']\n",
    "        for key_name in list(state_dict.keys()):\n",
    "            new_key = key_name.replace('backbone.','')\n",
    "            state_dict[new_key] = state_dict.pop(key_name)\n",
    "    else:\n",
    "        # AutoAugment or Advprop (different preprocessing)\n",
    "        url_map_ = url_map_advprop if advprop else url_map\n",
    "        state_dict = model_zoo.load_url(url_map_[model_name])\n",
    "\n",
    "    if load_fc:\n",
    "        state_dict.pop('head.weight')\n",
    "        ret = model.load_state_dict(state_dict, strict=False)\n",
    "        assert not ret.missing_keys, 'Missing keys when loading pretrained weights: {}'.format(ret.missing_keys)\n",
    "    else:\n",
    "        state_dict.pop('backbone._fc.weight')\n",
    "        state_dict.pop('backbone._fc.bias')\n",
    "        ret = model.load_state_dict(state_dict, strict=False)\n",
    "        assert set(ret.missing_keys) == set(\n",
    "            ['_fc.weight', '_fc.bias']), 'Missing keys when loading pretrained weights: {}'.format(ret.missing_keys)\n",
    "    assert not ret.unexpected_keys, 'Missing keys when loading pretrained weights: {}'.format(ret.unexpected_keys)\n",
    "\n",
    "    print('Loaded pretrained weights for {}'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage_labs/3030/BelyakovM/Face_attributes/Code/EfficientNet_B0_face_recognizer/FaceX-Zoo/backbone/backbone_def.py:32: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  backbone_conf = yaml.load(f)\n",
      "/storage_labs/3030/BelyakovM/Face_attributes/Code/EfficientNet_B0_face_recognizer/FaceX-Zoo/head/head_def.py:32: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  head_conf = yaml.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone param:\n",
      "{'width': 1.0, 'depth': 1.0, 'image_size': 112, 'drop_ratio': 0.2, 'out_h': 7, 'out_w': 7, 'feat_dim': 512}\n",
      "head param:\n",
      "{'feat_dim': 512, 'num_class': 72778, 'scale': 32, 'lamda': 70.0}\n",
      "Loaded pretrained weights for EfficientNet\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('/storage_labs/3030/BelyakovM/Face_attributes/Code/EfficientNet_B0_face_recognizer/FaceX-Zoo')\n",
    "from backbone.backbone_def import BackboneFactory\n",
    "from head.head_def import HeadFactory\n",
    "class FaceModel(torch.nn.Module):\n",
    "    \"\"\"Define a traditional face model which contains a backbone and a head.\n",
    "    \n",
    "    Attributes:\n",
    "        backbone(object): the backbone of face model.\n",
    "        head(object): the head of face model.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone_factory, head_factory):\n",
    "        \"\"\"Init face model by backbone factorcy and head factory.\n",
    "        \n",
    "        Args:\n",
    "            backbone_factory(object): produce a backbone according to config files.\n",
    "            head_factory(object): produce a head according to config files.\n",
    "        \"\"\"\n",
    "        super(FaceModel, self).__init__()\n",
    "        self.backbone = backbone_factory.get_backbone()\n",
    "        self.head = head_factory.get_head()\n",
    "        self.expression_head = Sequential(nn.Linear(in_features=512, out_features=30, bias=True),nn.Linear(in_features=30, out_features=7, bias=True))\n",
    "\n",
    "    def forward(self, data):\n",
    "        logits = self.backbone.forward(data)\n",
    "        expression = self.expression_head(logits)\n",
    "       \n",
    "        return logits,expression\n",
    "backbone_factory = BackboneFactory(args.backbone_type, args.backbone_conf_file)   \n",
    "head_factory = HeadFactory(args.head_type, args.head_conf_file)\n",
    "efficientnet_b0_pretrained_frozen_expressionhead2logits = FaceModel(backbone_factory, head_factory)\n",
    "load_pretrained_weights(efficientnet_b0_pretrained_frozen_expressionhead2logits.backbone,args.backbone_type,weights_path ='/storage_labs/3030/BelyakovM/Face_attributes/Code/EfficientNet_B0_face_recognizer/efficientnet_facerecognition_weights.pt',load_fc=True )\n",
    "efficientnet_b0_pretrained_frozen_expressionhead2logits=efficientnet_b0_pretrained_frozen_expressionhead2logits.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expression_head.0.weight\n",
      "expression_head.0.bias\n",
      "expression_head.1.weight\n",
      "expression_head.1.bias\n"
     ]
    }
   ],
   "source": [
    "for module in efficientnet_b0_pretrained_frozen_expressionhead2logits.backbone.modules():\n",
    "    if isinstance(module,nn.modules.BatchNorm1d):\n",
    "        module.eval()\n",
    "    if isinstance(module,nn.modules.BatchNorm2d):\n",
    "        module.eval()\n",
    "    if isinstance(module,nn.modules.BatchNorm3d):\n",
    "        module.eval()\n",
    "for i in efficientnet_b0_pretrained_frozen_expressionhead2logits.parameters():\n",
    "    i.requires_grad = False\n",
    "for param in efficientnet_b0_pretrained_frozen_expressionhead2logits.expression_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for name,param in efficientnet_b0_pretrained_frozen_expressionhead2logits.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(112),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(120),\n",
    "        transforms.CenterCrop(112),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset_expression_AffectPartly(torch.utils.data.Dataset):\n",
    "    def __init__(self,path_to_annotation,phase,dict_classes,transform = None,loader = default_loader):\n",
    "        annotation = open(path_to_annotation+str(phase)+'.txt','r')\n",
    "        self.annotations = annotation.readlines()\n",
    "        self.dict_classes = dict_classes\n",
    "        self.loader = loader\n",
    "        self.transform = transform\n",
    "        random.shuffle(self.annotations)\n",
    "        if len(self.annotations) == 0:\n",
    "            raise(RuntimeError(\"Found 0 images in subfolders of: \" + root + \"\\n\"))\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.annotations[index].split(';')\n",
    "        img = self.loader(path)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, self.dict_classes[target.replace('\\n','')]\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "annotations_path = '/storage_labs/3030/BelyakovM/Face_attributes/ds/db_BuevichP/emochon/AffecetNet_partly/original/annotation_'\n",
    "image_datasets_AffectPartly = {x: MyDataset_expression_AffectPartly(annotations_path,x,{'neutral':0, 'happiness':1, 'sadness':2, 'surprise':3, 'anger':4, 'disgust':5,'fear':6},\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset_expression_OZON(torch.utils.data.Dataset):\n",
    "    def __init__(self,path_to_annotation,phase,dict_classes,transform = None,loader = default_loader):\n",
    "        annotation = open(path_to_annotation,'r')\n",
    "        self.annotations = annotation.readlines()\n",
    "        self.dict_classes = dict_classes\n",
    "        self.phase = phase\n",
    "        self.loader = loader\n",
    "        self.transform = transform\n",
    "        random.shuffle(self.annotations)\n",
    "        split = int(0.2*len(self.annotations))\n",
    "        \n",
    "        if phase == 'train':\n",
    "            self.annotations = self.annotations[split:]\n",
    "        if phase == 'val':\n",
    "            self.annotations = self.annotations[:split]\n",
    "\n",
    "        if len(self.annotations) == 0:\n",
    "            raise(RuntimeError(\"Found 0 images in subfolders of: \" + root + \"\\n\"))\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.annotations[index].split(';')\n",
    "        img = self.loader(path)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, self.dict_classes[target.replace('\\n','')]\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "annotations_path_ozon = '/storage_labs/3030/BelyakovM/Face_attributes/ds/db_BuevichP/emochon/OZON_expressions_dataset/train_7expressions_annotation.txt'\n",
    "image_datasets_ozon = {x: MyDataset_expression_OZON(annotations_path_ozon,x,{'neutral':0, 'happy':1, 'sad':2, 'surprise':3, 'anger':4, 'disgust':5,'fear':6},\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets_AffectPartly_OZON = {'train': torch.utils.data.ConcatDataset([image_datasets_AffectPartly['train'],image_datasets_ozon['train']]),\n",
    "                                    'val_AffectPartly': image_datasets_AffectPartly['val'],'val_ozon':image_datasets_ozon['val']}\n",
    "dataloaders_AffectPartly_OZON = {x: torch.utils.data.DataLoader(image_datasets_AffectPartly_OZON[x], batch_size=63,\n",
    "                                             shuffle=True, num_workers=4) for x in ['train', 'val_AffectPartly','val_ozon']}\n",
    "\n",
    "dataset_sizes_AffectPartly_OZON = {x: image_datasets_AffectPartly_OZON[x].__len__() for x in ['train', 'val_AffectPartly','val_ozon']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler,dataloaders,dataset_sizes, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val_AffectPartly','val_ozon']:\n",
    "            model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)[1]\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val_ozon' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "optimizer_efficientnet_b0_pretrained_frozen_expressionhead2logits = optim.SGD(efficientnet_b0_pretrained_frozen_expressionhead2logits.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_efficientnet_b0_pretrained_frozen_expressionhead2logits, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.9350 Acc: 0.1649\n",
      "val_AffectPartly Loss: 1.9528 Acc: 0.1560\n",
      "val_ozon Loss: 1.9225 Acc: 0.1729\n",
      "\n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.9282 Acc: 0.1739\n",
      "val_AffectPartly Loss: 1.9485 Acc: 0.1686\n",
      "val_ozon Loss: 1.9180 Acc: 0.1789\n",
      "\n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.9247 Acc: 0.1824\n",
      "val_AffectPartly Loss: 1.9453 Acc: 0.1837\n",
      "val_ozon Loss: 1.9148 Acc: 0.1844\n",
      "\n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.9230 Acc: 0.1841\n",
      "val_AffectPartly Loss: 1.9422 Acc: 0.1820\n",
      "val_ozon Loss: 1.9118 Acc: 0.1913\n",
      "\n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.9223 Acc: 0.1860\n",
      "val_AffectPartly Loss: 1.9391 Acc: 0.1857\n",
      "val_ozon Loss: 1.9096 Acc: 0.1952\n",
      "\n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.9196 Acc: 0.1886\n",
      "val_AffectPartly Loss: 1.9386 Acc: 0.1829\n",
      "val_ozon Loss: 1.9067 Acc: 0.2006\n",
      "\n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.9184 Acc: 0.1896\n",
      "val_AffectPartly Loss: 1.9339 Acc: 0.1914\n",
      "val_ozon Loss: 1.9044 Acc: 0.1997\n",
      "\n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.9169 Acc: 0.1924\n",
      "val_AffectPartly Loss: 1.9338 Acc: 0.1951\n",
      "val_ozon Loss: 1.9039 Acc: 0.1991\n",
      "\n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.9177 Acc: 0.1930\n",
      "val_AffectPartly Loss: 1.9335 Acc: 0.1943\n",
      "val_ozon Loss: 1.9036 Acc: 0.2010\n",
      "\n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.9174 Acc: 0.1934\n",
      "val_AffectPartly Loss: 1.9333 Acc: 0.1951\n",
      "val_ozon Loss: 1.9034 Acc: 0.2016\n",
      "\n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.9170 Acc: 0.1923\n",
      "val_AffectPartly Loss: 1.9331 Acc: 0.1949\n",
      "val_ozon Loss: 1.9031 Acc: 0.2033\n",
      "\n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.9178 Acc: 0.1906\n",
      "val_AffectPartly Loss: 1.9329 Acc: 0.1949\n",
      "val_ozon Loss: 1.9028 Acc: 0.2019\n",
      "\n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.9169 Acc: 0.1938\n",
      "val_AffectPartly Loss: 1.9326 Acc: 0.1946\n",
      "val_ozon Loss: 1.9026 Acc: 0.2031\n",
      "\n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.9161 Acc: 0.1941\n",
      "val_AffectPartly Loss: 1.9323 Acc: 0.1943\n",
      "val_ozon Loss: 1.9023 Acc: 0.2027\n",
      "\n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.9172 Acc: 0.1913\n",
      "val_AffectPartly Loss: 1.9323 Acc: 0.1940\n",
      "val_ozon Loss: 1.9023 Acc: 0.2029\n",
      "\n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.9163 Acc: 0.1932\n",
      "val_AffectPartly Loss: 1.9323 Acc: 0.1940\n",
      "val_ozon Loss: 1.9023 Acc: 0.2025\n",
      "\n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.9175 Acc: 0.1914\n",
      "val_AffectPartly Loss: 1.9323 Acc: 0.1943\n",
      "val_ozon Loss: 1.9022 Acc: 0.2023\n",
      "\n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.9166 Acc: 0.1919\n",
      "val_AffectPartly Loss: 1.9322 Acc: 0.1943\n",
      "val_ozon Loss: 1.9022 Acc: 0.2023\n",
      "\n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.9163 Acc: 0.1922\n",
      "val_AffectPartly Loss: 1.9322 Acc: 0.1943\n",
      "val_ozon Loss: 1.9022 Acc: 0.2027\n",
      "\n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.9168 Acc: 0.1936\n",
      "val_AffectPartly Loss: 1.9322 Acc: 0.1943\n",
      "val_ozon Loss: 1.9022 Acc: 0.2029\n",
      "\n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 1.9180 Acc: 0.1928\n",
      "val_AffectPartly Loss: 1.9322 Acc: 0.1946\n",
      "val_ozon Loss: 1.9021 Acc: 0.2031\n",
      "\n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 1.9166 Acc: 0.1930\n",
      "val_AffectPartly Loss: 1.9322 Acc: 0.1946\n",
      "val_ozon Loss: 1.9021 Acc: 0.2031\n",
      "\n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.9163 Acc: 0.1930\n",
      "val_AffectPartly Loss: 1.9322 Acc: 0.1946\n",
      "val_ozon Loss: 1.9021 Acc: 0.2031\n",
      "\n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.9155 Acc: 0.1932\n",
      "val_AffectPartly Loss: 1.9321 Acc: 0.1946\n",
      "val_ozon Loss: 1.9021 Acc: 0.2031\n",
      "\n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.9169 Acc: 0.1922\n",
      "val_AffectPartly Loss: 1.9321 Acc: 0.1946\n",
      "val_ozon Loss: 1.9021 Acc: 0.2031\n",
      "\n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 1.9167 Acc: 0.1918\n",
      "val_AffectPartly Loss: 1.9321 Acc: 0.1946\n",
      "val_ozon Loss: 1.9021 Acc: 0.2031\n",
      "\n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.9165 Acc: 0.1932\n",
      "val_AffectPartly Loss: 1.9321 Acc: 0.1946\n",
      "val_ozon Loss: 1.9021 Acc: 0.2031\n",
      "\n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 1.9170 Acc: 0.1916\n",
      "val_AffectPartly Loss: 1.9321 Acc: 0.1946\n",
      "val_ozon Loss: 1.9021 Acc: 0.2033\n",
      "\n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 1.9165 Acc: 0.1942\n",
      "val_AffectPartly Loss: 1.9321 Acc: 0.1946\n",
      "val_ozon Loss: 1.9021 Acc: 0.2033\n",
      "\n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 1.9164 Acc: 0.1928\n",
      "val_AffectPartly Loss: 1.9321 Acc: 0.1946\n",
      "val_ozon Loss: 1.9021 Acc: 0.2033\n",
      "\n",
      "Training complete in 165m 46s\n",
      "Best val Acc: 0.203266\n"
     ]
    }
   ],
   "source": [
    "\n",
    "efficientnet_b0_pretrained_frozen_expressionhead2logits = train_model(efficientnet_b0_pretrained_frozen_expressionhead2logits, criterion,optimizer_efficientnet_b0_pretrained_frozen_expressionhead2logits, exp_lr_scheduler,dataloaders_AffectPartly_OZON,dataset_sizes_AffectPartly_OZON,\n",
    "                       num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(efficientnet_b0_pretrained_frozen_expressionhead2logits.state_dict(), '/storage_labs/3030/BelyakovM/Face_attributes/Saved_models/efficientnet_b0_pretrained_frozen_backbone_expressionhead2logits.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
