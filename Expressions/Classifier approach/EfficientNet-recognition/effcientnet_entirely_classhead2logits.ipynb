{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook aims to find out how accurate EfficientNet-b0 with Face recognition weights can converge when entirely trained on OZON-AffectNet composite dataset. Expressnion heah is mounted on 512 logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.datasets.folder import default_loader\n",
    "import math\n",
    "import collections\n",
    "from functools import partial\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import copy\n",
    "from torch.utils import model_zoo\n",
    "from torch.nn import Sequential, BatchNorm1d, BatchNorm2d, Dropout, Module, Linear\n",
    "import yaml\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = argparse.ArgumentParser(description='traditional_training for face recognition.')\n",
    "\n",
    "conf.add_argument(\"--backbone_type\", type = str,default = 'EfficientNet',\n",
    "                      help = \"Mobilefacenets, Resnet.\")\n",
    "conf.add_argument(\"--backbone_conf_file\", type = str ,default ='/storage_labs/3030/BelyakovM/Face_attributes/Code/EfficientNet_B0_face_recognizer/FaceX-Zoo/training_mode/backbone_conf.yaml', \n",
    "                      help = \"the path of backbone_conf.yaml.\")\n",
    "conf.add_argument(\"--head_type\", type = str ,default = 'AdaM-Softmax', \n",
    "                      help = \"mv-softmax, arcface, npc-face.\")\n",
    "conf.add_argument(\"--head_conf_file\", type = str ,default = '/storage_labs/3030/BelyakovM/Face_attributes/Code/EfficientNet_B0_face_recognizer/FaceX-Zoo/training_mode/head_conf.yaml', \n",
    "                      help = \"the path of head_conf.yaml.\")\n",
    "    \n",
    "args = conf.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_weights(model, model_name, weights_path=None, load_fc=True, advprop=False):\n",
    "    \"\"\"Loads pretrained weights from weights path or download using url.\n",
    "    Args:\n",
    "        model (Module): The whole model of efficientnet.\n",
    "        model_name (str): Model name of efficientnet.\n",
    "        weights_path (None or str):\n",
    "            str: path to pretrained weights file on the local disk.\n",
    "            None: use pretrained weights downloaded from the Internet.\n",
    "        load_fc (bool): Whether to load pretrained weights for fc layer at the end of the model.\n",
    "        advprop (bool): Whether to load pretrained weights\n",
    "                        trained with advprop (valid when weights_path is None).\n",
    "    \"\"\"\n",
    "    if isinstance(weights_path, str):\n",
    "        state_dict = torch.load(weights_path)['state_dict']\n",
    "        for key_name in list(state_dict.keys()):\n",
    "            new_key = key_name.replace('backbone.','')\n",
    "            state_dict[new_key] = state_dict.pop(key_name)\n",
    "    else:\n",
    "        # AutoAugment or Advprop (different preprocessing)\n",
    "        url_map_ = url_map_advprop if advprop else url_map\n",
    "        state_dict = model_zoo.load_url(url_map_[model_name])\n",
    "\n",
    "    if load_fc:\n",
    "        state_dict.pop('head.weight')\n",
    "        ret = model.load_state_dict(state_dict, strict=False)\n",
    "        assert not ret.missing_keys, 'Missing keys when loading pretrained weights: {}'.format(ret.missing_keys)\n",
    "    else:\n",
    "        state_dict.pop('backbone._fc.weight')\n",
    "        state_dict.pop('backbone._fc.bias')\n",
    "        ret = model.load_state_dict(state_dict, strict=False)\n",
    "        assert set(ret.missing_keys) == set(\n",
    "            ['_fc.weight', '_fc.bias']), 'Missing keys when loading pretrained weights: {}'.format(ret.missing_keys)\n",
    "    assert not ret.unexpected_keys, 'Missing keys when loading pretrained weights: {}'.format(ret.unexpected_keys)\n",
    "\n",
    "    print('Loaded pretrained weights for {}'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage_labs/3030/BelyakovM/Face_attributes/Code/EfficientNet_B0_face_recognizer/FaceX-Zoo/backbone/backbone_def.py:32: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  backbone_conf = yaml.load(f)\n",
      "/storage_labs/3030/BelyakovM/Face_attributes/Code/EfficientNet_B0_face_recognizer/FaceX-Zoo/head/head_def.py:32: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  head_conf = yaml.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone param:\n",
      "{'width': 1.0, 'depth': 1.0, 'image_size': 112, 'drop_ratio': 0.2, 'out_h': 7, 'out_w': 7, 'feat_dim': 512}\n",
      "head param:\n",
      "{'feat_dim': 512, 'num_class': 72778, 'scale': 32, 'lamda': 70.0}\n",
      "Loaded pretrained weights for EfficientNet\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('/storage_labs/3030/BelyakovM/Face_attributes/Code/EfficientNet_B0_face_recognizer/FaceX-Zoo')\n",
    "from backbone.backbone_def import BackboneFactory\n",
    "from head.head_def import HeadFactory\n",
    "class FaceModel(torch.nn.Module):\n",
    "    \"\"\"Define a traditional face model which contains a backbone and a head.\n",
    "    \n",
    "    Attributes:\n",
    "        backbone(object): the backbone of face model.\n",
    "        head(object): the head of face model.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone_factory, head_factory):\n",
    "        \"\"\"Init face model by backbone factorcy and head factory.\n",
    "        \n",
    "        Args:\n",
    "            backbone_factory(object): produce a backbone according to config files.\n",
    "            head_factory(object): produce a head according to config files.\n",
    "        \"\"\"\n",
    "        super(FaceModel, self).__init__()\n",
    "        self.backbone = backbone_factory.get_backbone()\n",
    "        self.head = head_factory.get_head()\n",
    "        self.expression_head = Sequential(nn.Linear(in_features=512, out_features=30, bias=True),nn.Linear(in_features=30, out_features=7, bias=True))\n",
    "\n",
    "    def forward(self, data):\n",
    "        logits = self.backbone.forward(data)\n",
    "        expression = self.expression_head(logits)\n",
    "       \n",
    "        return logits,expression\n",
    "backbone_factory = BackboneFactory(args.backbone_type, args.backbone_conf_file)   \n",
    "head_factory = HeadFactory(args.head_type, args.head_conf_file)\n",
    "efficientnet_b0_pretrained_entirely_expressionhead2logits = FaceModel(backbone_factory, head_factory)\n",
    "load_pretrained_weights(efficientnet_b0_pretrained_entirely_expressionhead2logits.backbone,args.backbone_type,weights_path ='/storage_labs/3030/BelyakovM/Face_attributes/Code/EfficientNet_B0_face_recognizer/efficientnet_facerecognition_weights.pt',load_fc=True )\n",
    "efficientnet_b0_pretrained_entirely_expressionhead2logits=efficientnet_b0_pretrained_entirely_expressionhead2logits.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(112),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(120),\n",
    "        transforms.CenterCrop(112),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Datasets.MyDataset_expression_AffectPartly import MyDataset_expression_AffectPartly\n",
    "    \n",
    "annotations_path = '/storage_labs/3030/BelyakovM/Face_attributes/ds/db_BuevichP/emochon/AffecetNet_partly/original/annotation_'\n",
    "image_datasets_AffectPartly = {x: MyDataset_expression_AffectPartly(annotations_path,x,{'neutral':0, 'happiness':1, 'sadness':2, 'surprise':3, 'anger':4, 'disgust':5,'fear':6},\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Datasets.MyDataset_expression_OZON import MyDataset_expression_OZON\n",
    "    \n",
    "annotations_path_ozon = '/storage_labs/3030/BelyakovM/Face_attributes/ds/db_BuevichP/emochon/OZON_expressions_dataset/train_7expressions_annotation.txt'\n",
    "image_datasets_ozon = {x: MyDataset_expression_OZON(annotations_path_ozon,x,{'neutral':0, 'happy':1, 'sad':2, 'surprise':3, 'anger':4, 'disgust':5,'fear':6},\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets_AffectPartly_OZON = {'train': torch.utils.data.ConcatDataset([image_datasets_AffectPartly['train'],image_datasets_ozon['train']]),\n",
    "                                    'val_AffectPartly': image_datasets_AffectPartly['val'],'val_ozon':image_datasets_ozon['val']}\n",
    "dataloaders_AffectPartly_OZON = {x: torch.utils.data.DataLoader(image_datasets_AffectPartly_OZON[x], batch_size=63,\n",
    "                                             shuffle=True, num_workers=4) for x in ['train', 'val_AffectPartly','val_ozon']}\n",
    "\n",
    "dataset_sizes_AffectPartly_OZON = {x: image_datasets_AffectPartly_OZON[x].__len__() for x in ['train', 'val_AffectPartly','val_ozon']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler,dataloaders,dataset_sizes, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val_AffectPartly','val_ozon']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval() \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)[1]\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val_ozon' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_efficientnet_b0_pretrained_entirely_expressionhead2logits = optim.SGD(efficientnet_b0_pretrained_entirely_expressionhead2logits.parameters(), lr=0.00001, momentum=0.9)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_efficientnet_b0_pretrained_entirely_expressionhead2logits, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.9678 Acc: 0.1207\n",
      "val_AffectPartly Loss: 1.9554 Acc: 0.1537\n",
      "val_ozon Loss: 1.9694 Acc: 0.1254\n",
      "\n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.9657 Acc: 0.1244\n",
      "val_AffectPartly Loss: 1.9533 Acc: 0.1529\n",
      "val_ozon Loss: 1.9662 Acc: 0.1291\n",
      "\n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.9645 Acc: 0.1224\n",
      "val_AffectPartly Loss: 1.9514 Acc: 0.1529\n",
      "val_ozon Loss: 1.9631 Acc: 0.1348\n",
      "\n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.9623 Acc: 0.1299\n",
      "val_AffectPartly Loss: 1.9493 Acc: 0.1589\n",
      "val_ozon Loss: 1.9598 Acc: 0.1398\n",
      "\n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.9602 Acc: 0.1304\n",
      "val_AffectPartly Loss: 1.9472 Acc: 0.1591\n",
      "val_ozon Loss: 1.9565 Acc: 0.1442\n",
      "\n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.9594 Acc: 0.1337\n",
      "val_AffectPartly Loss: 1.9457 Acc: 0.1617\n",
      "val_ozon Loss: 1.9541 Acc: 0.1500\n",
      "\n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.9566 Acc: 0.1377\n",
      "val_AffectPartly Loss: 1.9440 Acc: 0.1626\n",
      "val_ozon Loss: 1.9516 Acc: 0.1523\n",
      "\n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.9559 Acc: 0.1404\n",
      "val_AffectPartly Loss: 1.9439 Acc: 0.1646\n",
      "val_ozon Loss: 1.9507 Acc: 0.1537\n",
      "\n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.9559 Acc: 0.1407\n",
      "val_AffectPartly Loss: 1.9438 Acc: 0.1649\n",
      "val_ozon Loss: 1.9508 Acc: 0.1529\n",
      "\n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.9551 Acc: 0.1396\n",
      "val_AffectPartly Loss: 1.9434 Acc: 0.1651\n",
      "val_ozon Loss: 1.9505 Acc: 0.1526\n",
      "\n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.9561 Acc: 0.1389\n",
      "val_AffectPartly Loss: 1.9431 Acc: 0.1606\n",
      "val_ozon Loss: 1.9502 Acc: 0.1550\n",
      "\n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.9559 Acc: 0.1427\n",
      "val_AffectPartly Loss: 1.9431 Acc: 0.1654\n",
      "val_ozon Loss: 1.9502 Acc: 0.1539\n",
      "\n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.9548 Acc: 0.1437\n",
      "val_AffectPartly Loss: 1.9431 Acc: 0.1651\n",
      "val_ozon Loss: 1.9492 Acc: 0.1577\n",
      "\n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.9554 Acc: 0.1409\n",
      "val_AffectPartly Loss: 1.9428 Acc: 0.1660\n",
      "val_ozon Loss: 1.9495 Acc: 0.1570\n",
      "\n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.9556 Acc: 0.1428\n",
      "val_AffectPartly Loss: 1.9426 Acc: 0.1646\n",
      "val_ozon Loss: 1.9496 Acc: 0.1565\n",
      "\n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.9548 Acc: 0.1422\n",
      "val_AffectPartly Loss: 1.9428 Acc: 0.1660\n",
      "val_ozon Loss: 1.9492 Acc: 0.1572\n",
      "\n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.9556 Acc: 0.1413\n",
      "val_AffectPartly Loss: 1.9431 Acc: 0.1654\n",
      "val_ozon Loss: 1.9499 Acc: 0.1565\n",
      "\n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.9557 Acc: 0.1400\n",
      "val_AffectPartly Loss: 1.9427 Acc: 0.1651\n",
      "val_ozon Loss: 1.9497 Acc: 0.1559\n",
      "\n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.9551 Acc: 0.1404\n",
      "val_AffectPartly Loss: 1.9429 Acc: 0.1649\n",
      "val_ozon Loss: 1.9492 Acc: 0.1573\n",
      "\n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.9558 Acc: 0.1412\n",
      "val_AffectPartly Loss: 1.9427 Acc: 0.1654\n",
      "val_ozon Loss: 1.9491 Acc: 0.1559\n",
      "\n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 1.9554 Acc: 0.1411\n",
      "val_AffectPartly Loss: 1.9427 Acc: 0.1654\n",
      "val_ozon Loss: 1.9490 Acc: 0.1554\n",
      "\n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 1.9547 Acc: 0.1413\n",
      "val_AffectPartly Loss: 1.9428 Acc: 0.1643\n",
      "val_ozon Loss: 1.9497 Acc: 0.1559\n",
      "\n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.9555 Acc: 0.1408\n",
      "val_AffectPartly Loss: 1.9426 Acc: 0.1651\n",
      "val_ozon Loss: 1.9499 Acc: 0.1551\n",
      "\n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.9550 Acc: 0.1421\n",
      "val_AffectPartly Loss: 1.9429 Acc: 0.1651\n",
      "val_ozon Loss: 1.9498 Acc: 0.1559\n",
      "\n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.9549 Acc: 0.1420\n",
      "val_AffectPartly Loss: 1.9424 Acc: 0.1677\n",
      "val_ozon Loss: 1.9490 Acc: 0.1563\n",
      "\n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 1.9554 Acc: 0.1402\n",
      "val_AffectPartly Loss: 1.9427 Acc: 0.1640\n",
      "val_ozon Loss: 1.9493 Acc: 0.1568\n",
      "\n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.9556 Acc: 0.1425\n",
      "val_AffectPartly Loss: 1.9427 Acc: 0.1666\n",
      "val_ozon Loss: 1.9497 Acc: 0.1554\n",
      "\n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 1.9550 Acc: 0.1415\n",
      "val_AffectPartly Loss: 1.9429 Acc: 0.1663\n",
      "val_ozon Loss: 1.9493 Acc: 0.1566\n",
      "\n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 1.9556 Acc: 0.1420\n",
      "val_AffectPartly Loss: 1.9426 Acc: 0.1666\n",
      "val_ozon Loss: 1.9494 Acc: 0.1561\n",
      "\n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 1.9546 Acc: 0.1426\n",
      "val_AffectPartly Loss: 1.9428 Acc: 0.1643\n",
      "val_ozon Loss: 1.9491 Acc: 0.1571\n",
      "\n",
      "Training complete in 170m 51s\n",
      "Best val Acc: 0.157689\n"
     ]
    }
   ],
   "source": [
    "\n",
    "efficientnet_b0_pretrained_entirely_expressionhead2logits = train_model(efficientnet_b0_pretrained_entirely_expressionhead2logits, criterion,optimizer_efficientnet_b0_pretrained_entirely_expressionhead2logits, exp_lr_scheduler,dataloaders_AffectPartly_OZON,dataset_sizes_AffectPartly_OZON,\n",
    "                       num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(efficientnet_b0_pretrained_entirely_expressionhead2logits.state_dict(), '/storage_labs/3030/BelyakovM/Face_attributes/Saved_models/efficientnet_b0_pretrained_entirely_trained_expressionhead2logits.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
